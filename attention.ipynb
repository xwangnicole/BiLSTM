{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-26T15:09:07.138514Z",
     "start_time": "2019-07-26T15:09:07.107269Z"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "import pandas as pd # (e.g. pd.read_csv)\n",
    "import gc\n",
    "import re\n",
    "\n",
    "from keras.preprocessing import text, sequence\n",
    "from keras import backend as K\n",
    "from keras.models import Model\n",
    "from keras.layers import Input, Dense, Embedding, SpatialDropout1D, add, concatenate\n",
    "from keras.layers import LSTM, Bidirectional, GlobalMaxPooling1D, GlobalAveragePooling1D\n",
    "from keras.preprocessing import text, sequence\n",
    "from keras.callbacks import LearningRateScheduler\n",
    "from keras.engine.topology import Layer\n",
    "from keras import initializers, regularizers, constraints, optimizers, layers\n",
    "from tqdm._tqdm_notebook import tqdm_notebook as tqdm\n",
    "import pickle\n",
    "tqdm.pandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-26T15:09:08.283790Z",
     "start_time": "2019-07-26T15:09:08.111902Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda2\\lib\\site-packages\\ipykernel_launcher.py:4: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  after removing the cwd from sys.path.\n",
      "C:\\ProgramData\\Anaconda2\\lib\\site-packages\\ipykernel_launcher.py:8: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  \n"
     ]
    }
   ],
   "source": [
    "test=pd.read_csv('C:/Users/nwang/Desktop/nlp/code/test_emoji.csv',names=['text','target'])\n",
    "test['a']=[1]*54+[1,1]\n",
    "test['b']=[1]*54+[1,1]\n",
    "test['target'][test['target']==4]=3\n",
    "train=pd.read_csv('C:/Users/nwang/Desktop/nlp/code/train_emoji.csv',names=['text','target','a','b'])\n",
    "train['a']=[1]*130+[1,1]\n",
    "train['b']=[1]*130+[1,1]\n",
    "train['target'][train['target']==4]=3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-26T15:14:32.939182Z",
     "start_time": "2019-07-26T15:14:32.907923Z"
    }
   },
   "outputs": [],
   "source": [
    "train_df=pd.concat([train[['a','b','text']],\\\n",
    "           pd.DataFrame(np.eye(4)[np.array(train['target']).reshape(-1)], columns=['t1','t2','t3','t4'])],axis=1)\n",
    "test_df=pd.concat([test[['a','b','text']],\\\n",
    "           pd.DataFrame(np.eye(4)[np.array(test['target']).reshape(-1)], columns=['t1','t2','t3','t4'])],axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-26T15:14:34.531619Z",
     "start_time": "2019-07-26T15:14:34.516017Z"
    }
   },
   "outputs": [],
   "source": [
    "EMBEDDING_PATHS = ['E:/data/pkl/crawl-300d-2M.pkl',\\\n",
    "                   'E:/data/pkl/glove.840B.300d.pkl']\n",
    "\n",
    "\n",
    "NUM_MODELS = 2 \n",
    "BATCH_SIZE = 512 \n",
    "LSTM_UNITS = 128 \n",
    "DENSE_HIDDEN_UNITS = 4*LSTM_UNITS \n",
    "EPOCHS = 4 \n",
    "MAX_LEN = 220\n",
    "\n",
    "AUX_COLUMNS = ['t1','t2','t3','t4', 'a','b']\n",
    "AUX_COLUMNS_SUB = ['a','b']\n",
    "TEXT_COLUMN = 'text'\n",
    "TARGET_COLUMN = ['t1','t2','t3','t4']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-26T15:09:20.112705Z",
     "start_time": "2019-07-26T15:09:20.081465Z"
    }
   },
   "outputs": [],
   "source": [
    "def handle_punctuation(text):\n",
    "    text = text.translate(REMOVE_DICT)\n",
    "    text = text.translate(ISOLATE_DICT)\n",
    "    return text\n",
    "\n",
    "def clean_contractions(text, mapping=CONTRACTION_MAPPING):\n",
    "    '''\n",
    "    Expand contractions\n",
    "    '''\n",
    "    specials = [\"‚Äô\", \"‚Äò\", \"¬¥\", \"`\"]\n",
    "    for s in specials:\n",
    "        text = text.replace(s, \"'\")\n",
    "    text = ' '.join([mapping[t] if t in mapping else t for t in text.split(\" \")])\n",
    "    return text\n",
    "def preprocess(x):\n",
    "    x = handle_punctuation(x)\n",
    "#     x = clean_contractions(x)\n",
    "    return x\n",
    "\n",
    "def get_coefs(word, *arr):\n",
    "    \"\"\"\n",
    "    Get word, word_embedding from a pretrained embedding file\n",
    "    \"\"\"\n",
    "    return word, np.asarray(arr,dtype='float32')\n",
    "\n",
    "def load_embeddings(path):\n",
    "    if path.split('.')[-1] in ['txt','vec']: \n",
    "        with open(path,'rb') as f:\n",
    "            for line in f:\n",
    "                return dict(get_coefs(*line.strip().split(' ')) for line in f)    \n",
    "    if path.split('.')[-1] =='pkl': \n",
    "        with open(path,'rb') as f:\n",
    "            return pickle.load(f)\n",
    "        \n",
    "def build_matrix(word_index, path):\n",
    "    \"\"\"\n",
    "    Here we take each word we've tokenized in our text corpus\n",
    "    for each word we look up in the pre-trained embedding.\n",
    "    Each row in this matrix is a corpus word's embedding.\n",
    "    \"\"\"\n",
    "    embedding_index = load_embeddings(path)\n",
    "    embedding_matrix = np.zeros((len(word_index)+1, 300))\n",
    "    for word, i in word_index.items():\n",
    "        try:\n",
    "            embedding_matrix[i] = embedding_index[word]\n",
    "        except KeyError:\n",
    "            pass\n",
    "    return embedding_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-26T15:09:22.454720Z",
     "start_time": "2019-07-26T15:09:22.423481Z"
    }
   },
   "outputs": [],
   "source": [
    "class Attention(Layer):\n",
    "    def __init__(self, step_dim,\n",
    "                 W_regularizer=None, b_regularizer=None,\n",
    "                 W_constraint=None, b_constraint=None,\n",
    "                 bias=True, **kwargs):\n",
    "        self.supports_masking = True\n",
    "        self.init = initializers.get('glorot_uniform')\n",
    "\n",
    "        self.W_regularizer = regularizers.get(W_regularizer)\n",
    "        self.b_regularizer = regularizers.get(b_regularizer)\n",
    "\n",
    "        self.W_constraint = constraints.get(W_constraint)\n",
    "        self.b_constraint = constraints.get(b_constraint)\n",
    "\n",
    "        self.bias = bias\n",
    "        self.step_dim = step_dim\n",
    "        self.features_dim = 0\n",
    "        super(Attention, self).__init__(**kwargs)\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        assert len(input_shape) == 3\n",
    "\n",
    "        self.W = self.add_weight((input_shape[-1],),\n",
    "                                 initializer=self.init,\n",
    "                                 name='{}_W'.format(self.name),\n",
    "                                 regularizer=self.W_regularizer,\n",
    "                                 constraint=self.W_constraint)\n",
    "        self.features_dim = input_shape[-1]\n",
    "\n",
    "        if self.bias:\n",
    "            self.b = self.add_weight((input_shape[1],),\n",
    "                                     initializer='zero',\n",
    "                                     name='{}_b'.format(self.name),\n",
    "                                     regularizer=self.b_regularizer,\n",
    "                                     constraint=self.b_constraint)\n",
    "        else:\n",
    "            self.b = None\n",
    "\n",
    "        self.built = True\n",
    "\n",
    "    def compute_mask(self, input, input_mask=None):\n",
    "        return None\n",
    "\n",
    "    def call(self, x, mask=None):\n",
    "        features_dim = self.features_dim\n",
    "        step_dim = self.step_dim\n",
    "\n",
    "        eij = K.reshape(K.dot(K.reshape(x, (-1, features_dim)),\n",
    "                        K.reshape(self.W, (features_dim, 1))), (-1, step_dim))\n",
    "\n",
    "        if self.bias:\n",
    "            eij += self.b\n",
    "\n",
    "        eij = K.tanh(eij)\n",
    "\n",
    "        a = K.exp(eij)\n",
    "\n",
    "        if mask is not None:\n",
    "            a *= K.cast(mask, K.floatx())\n",
    "\n",
    "        a /= K.cast(K.sum(a, axis=1, keepdims=True) + K.epsilon(), K.floatx())\n",
    "\n",
    "        a = K.expand_dims(a)\n",
    "        weighted_input = x * a\n",
    "        return K.sum(weighted_input, axis=1)\n",
    "\n",
    "    def compute_output_shape(self, input_shape):\n",
    "        return input_shape[0],  self.features_dim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-26T15:09:28.231444Z",
     "start_time": "2019-07-26T15:09:28.200195Z"
    }
   },
   "outputs": [],
   "source": [
    "def build_model(embedding_matrix, num_aux_targets):#, loss_weight):\n",
    "    \"\"\"\n",
    "    embedding layer\n",
    "    droput layer\n",
    "    2 * bidirectional LSTM layers\n",
    "    2 * pooling layers\n",
    "    2 dense layers\n",
    "    1 softmax layer\n",
    "    \"\"\"\n",
    "    words = Input(shape=(MAX_LEN,)) \n",
    "    #Embedding layer takes variable size input\n",
    "    x = Embedding(*embedding_matrix.shape, weights = [embedding_matrix], trainable=False)(words)\n",
    "    x = SpatialDropout1D(0.2)(x)\n",
    "    x = Bidirectional(LSTM(LSTM_UNITS,return_sequences=True))(x)\n",
    "    x = Bidirectional(LSTM(LSTM_UNITS,return_sequences=True))(x)\n",
    "    \n",
    "    att = Attention(MAX_LEN)(x)\n",
    "    hidden = concatenate([ \n",
    "        GlobalMaxPooling1D()(x),\n",
    "        GlobalAveragePooling1D()(x)\n",
    "        ])\n",
    "    hidden = add([hidden, Dense(DENSE_HIDDEN_UNITS, activation='relu')(hidden)])\n",
    "    hidden = add([hidden, Dense(DENSE_HIDDEN_UNITS, activation='relu')(hidden)])\n",
    "    result = Dense(4, activation='sigmoid')(hidden)\n",
    "    aux_result =Dense(num_aux_targets, activation='sigmoid')(hidden)\n",
    "\n",
    "    model = Model(inputs =words, outputs =[result, aux_result])\n",
    "    model.compile(loss='binary_crossentropy', optimizer='adam')\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-26T15:14:38.669518Z",
     "start_time": "2019-07-26T15:14:38.544511Z"
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "eb0d07f44a714b1981127d3fe691f7cb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=132), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "73242ceea525482c9d4661b179c751ee",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=56), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "15"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df['text'] = train_df['text'].progress_apply(lambda x:preprocess(x))\n",
    "test_df['text'] = test_df['text'].progress_apply(lambda x:preprocess(x))\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-26T15:14:44.586097Z",
     "start_time": "2019-07-26T15:14:44.570469Z"
    }
   },
   "outputs": [],
   "source": [
    "x_train = train_df[TEXT_COLUMN].astype(str)\n",
    "y_train = train_df[TARGET_COLUMN].values\n",
    "y_aux_train = train_df[AUX_COLUMNS].values\n",
    "x_test = test_df[TEXT_COLUMN].astype(str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-26T15:14:46.841461Z",
     "start_time": "2019-07-26T15:14:46.810206Z"
    }
   },
   "outputs": [],
   "source": [
    "tokenizer = text.Tokenizer(filters = CHARS_TO_REMOVE)\n",
    "tokenizer.fit_on_texts(list(x_train)+ list(x_test))\n",
    "\n",
    "x_train = tokenizer.texts_to_sequences(x_train)\n",
    "x_test = tokenizer.texts_to_sequences(x_test)\n",
    "\n",
    "x_train = sequence.pad_sequences(x_train,maxlen=MAX_LEN)\n",
    "x_test= sequence.pad_sequences(x_test, maxlen=MAX_LEN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-26T15:17:33.868134Z",
     "start_time": "2019-07-26T15:17:33.302908Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda2\\lib\\site-packages\\ipykernel_launcher.py:6: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  \n",
      "C:\\ProgramData\\Anaconda2\\lib\\site-packages\\ipykernel_launcher.py:7: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  import sys\n"
     ]
    }
   ],
   "source": [
    "st= train_df[TARGET_COLUMN].shape[0]\n",
    "stpd= train_df[TARGET_COLUMN].sum(axis=0)\n",
    "train_weight=np.ones()\n",
    "\n",
    "for t in TARGET_COLUMN:\n",
    "    train_weight[t][train_weight[t]!=0]=float(st/stpd.loc[t,])\n",
    "    train_weight[t][train_weight[t]==0]=float(st/(st-stpd.loc[t,]))\n",
    "    \n",
    "# Initialize weights\n",
    "sample_weights = np.ones(len(x_train), dtype=np.float32)\n",
    "sample_weights += 20-train_df[AUX_COLUMNS_SUB].sum(axis=1)\n",
    "sample_weights/=sample_weights.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-26T15:29:38.565304Z",
     "start_time": "2019-07-26T15:29:38.549679Z"
    }
   },
   "outputs": [],
   "source": [
    "sample_weights = np.ones(train_df[TARGET_COLUMN].shape[0], dtype=np.float32)\n",
    "sample_weights *= train.groupby(['target'])['target'].transform('count')\n",
    "sample_weights/=sample_weights.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-26T15:36:42.761940Z",
     "start_time": "2019-07-26T15:36:42.730684Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "list"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type([sample_weights.values,sample_weights.values])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-26T14:26:17.853755Z",
     "start_time": "2019-07-26T14:25:38.217938Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embedding matrix shape: (313, 600)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "7"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embedding_matrix = np.concatenate([build_matrix(tokenizer.word_index,f) for f in EMBEDDING_PATHS], axis =-1)\n",
    "print(\"Embedding matrix shape:\", embedding_matrix.shape)\n",
    "del train_df, tokenizer\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-26T02:57:54.160642Z",
     "start_time": "2019-07-26T02:51:10.645034Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1\n",
      "132/132 [==============================] - 47s 357ms/step - loss: 1.4022 - dense_43_loss: 0.6942 - dense_44_loss: 0.7081\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x176a1804668>"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1\n",
      "132/132 [==============================] - 29s 223ms/step - loss: 1.3043 - dense_43_loss: 0.6504 - dense_44_loss: 0.6539\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x176a1804cc0>"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1\n",
      "132/132 [==============================] - 30s 225ms/step - loss: 1.2301 - dense_43_loss: 0.6185 - dense_44_loss: 0.6116\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x176a1804828>"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1\n",
      "132/132 [==============================] - 30s 228ms/step - loss: 1.1890 - dense_43_loss: 0.6016 - dense_44_loss: 0.5874\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x176ac04aac8>"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "48"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1\n",
      "132/132 [==============================] - 49s 374ms/step - loss: 1.3848 - dense_47_loss: 0.7028 - dense_48_loss: 0.6820\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x176afe20f98>"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1\n",
      "132/132 [==============================] - 31s 234ms/step - loss: 1.2425 - dense_47_loss: 0.6432 - dense_48_loss: 0.5993\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x176afe20eb8>"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1\n",
      "132/132 [==============================] - 32s 239ms/step - loss: 1.1641 - dense_47_loss: 0.6083 - dense_48_loss: 0.5559\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x176afe42da0>"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1\n",
      "132/132 [==============================] - 32s 240ms/step - loss: 1.1151 - dense_47_loss: 0.5892 - dense_48_loss: 0.5258\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x176a1804828>"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "152"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done\n"
     ]
    }
   ],
   "source": [
    "checkpoint_predictions = []\n",
    "weights = []\n",
    "for model_idx in range(NUM_MODELS):\n",
    "    #Passes embedding matrix and aux outputs shape\n",
    "    model = build_model(embedding_matrix, y_aux_train.shape[-1]) #1/sample_weights.mean())\n",
    "    for global_epoch in range(EPOCHS):\n",
    "        model.fit(\n",
    "            x_train,\n",
    "            [y_train, y_aux_train],\n",
    "            batch_size=BATCH_SIZE,\n",
    "            epochs=1,\n",
    "            verbose=1,\n",
    "            sample_weight=[sample_weights.values, np.ones_like(sample_weights)],\n",
    "            callbacks = [\n",
    "                LearningRateScheduler(lambda _: 1e-3*(0.55**global_epoch)) # Decayed learning rate make smaller\n",
    "                ]\n",
    "        )\n",
    "#         model.save_weights(\"model_%d_%d.h5\" % (model_idx, global_epoch)) # Save model weights\n",
    "        checkpoint_predictions.append(model.predict(x_test, batch_size=2048)[0].flatten())\n",
    "        pre=model.predict(x_test,batch_size=2048)[0]\n",
    "        weights.append(2 ** global_epoch)\n",
    "    del model # If a model didn't get deleted Keras will continue training it eventhough build_model() was used to initialize a model\n",
    "    gc.collect() # It's a good practice to use gc.collect() once the training is done to free up RAM\n",
    "print('done')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-26T03:29:37.971497Z",
     "start_time": "2019-07-26T02:59:25.473823Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "132/132 [==============================] - 54s 407ms/step - loss: 1.4183 - dense_51_loss: 0.6993 - dense_52_loss: 0.7189\n",
      "Epoch 2/50\n",
      "132/132 [==============================] - 34s 260ms/step - loss: 1.3919 - dense_51_loss: 0.6880 - dense_52_loss: 0.7039\n",
      "Epoch 3/50\n",
      "132/132 [==============================] - 34s 258ms/step - loss: 1.3687 - dense_51_loss: 0.6790 - dense_52_loss: 0.6897\n",
      "Epoch 4/50\n",
      "132/132 [==============================] - 35s 263ms/step - loss: 1.3451 - dense_51_loss: 0.6678 - dense_52_loss: 0.6773\n",
      "Epoch 5/50\n",
      "132/132 [==============================] - 35s 262ms/step - loss: 1.3216 - dense_51_loss: 0.6579 - dense_52_loss: 0.6637\n",
      "Epoch 6/50\n",
      "132/132 [==============================] - 34s 261ms/step - loss: 1.2993 - dense_51_loss: 0.6480 - dense_52_loss: 0.6513\n",
      "Epoch 7/50\n",
      "132/132 [==============================] - 35s 263ms/step - loss: 1.2752 - dense_51_loss: 0.6377 - dense_52_loss: 0.6376\n",
      "Epoch 8/50\n",
      "132/132 [==============================] - 35s 265ms/step - loss: 1.2549 - dense_51_loss: 0.6293 - dense_52_loss: 0.6255\n",
      "Epoch 9/50\n",
      "132/132 [==============================] - 35s 267ms/step - loss: 1.2331 - dense_51_loss: 0.6201 - dense_52_loss: 0.6130\n",
      "Epoch 10/50\n",
      "132/132 [==============================] - 35s 267ms/step - loss: 1.2042 - dense_51_loss: 0.6077 - dense_52_loss: 0.5965\n",
      "Epoch 11/50\n",
      "132/132 [==============================] - 35s 268ms/step - loss: 1.1783 - dense_51_loss: 0.5967 - dense_52_loss: 0.5817\n",
      "Epoch 12/50\n",
      "132/132 [==============================] - 36s 269ms/step - loss: 1.1538 - dense_51_loss: 0.5877 - dense_52_loss: 0.5661\n",
      "Epoch 13/50\n",
      "132/132 [==============================] - 35s 268ms/step - loss: 1.1204 - dense_51_loss: 0.5740 - dense_52_loss: 0.5464\n",
      "Epoch 14/50\n",
      "132/132 [==============================] - 35s 266ms/step - loss: 1.0989 - dense_51_loss: 0.5667 - dense_52_loss: 0.5322\n",
      "Epoch 15/50\n",
      "132/132 [==============================] - 35s 265ms/step - loss: 1.0697 - dense_51_loss: 0.5564 - dense_52_loss: 0.5133\n",
      "Epoch 16/50\n",
      "132/132 [==============================] - 35s 266ms/step - loss: 1.0426 - dense_51_loss: 0.5468 - dense_52_loss: 0.4957\n",
      "Epoch 17/50\n",
      "132/132 [==============================] - 35s 264ms/step - loss: 1.0184 - dense_51_loss: 0.5401 - dense_52_loss: 0.4784\n",
      "Epoch 18/50\n",
      "132/132 [==============================] - 35s 267ms/step - loss: 0.9927 - dense_51_loss: 0.5339 - dense_52_loss: 0.4588\n",
      "Epoch 19/50\n",
      "132/132 [==============================] - 35s 267ms/step - loss: 0.9787 - dense_51_loss: 0.5329 - dense_52_loss: 0.4458\n",
      "Epoch 20/50\n",
      "132/132 [==============================] - 35s 266ms/step - loss: 0.9642 - dense_51_loss: 0.5332 - dense_52_loss: 0.4310\n",
      "Epoch 21/50\n",
      "132/132 [==============================] - 35s 267ms/step - loss: 0.9541 - dense_51_loss: 0.5330 - dense_52_loss: 0.4211\n",
      "Epoch 22/50\n",
      "132/132 [==============================] - 35s 266ms/step - loss: 0.9422 - dense_51_loss: 0.5341 - dense_52_loss: 0.4081\n",
      "Epoch 23/50\n",
      "132/132 [==============================] - 35s 267ms/step - loss: 0.9356 - dense_51_loss: 0.5351 - dense_52_loss: 0.4005\n",
      "Epoch 24/50\n",
      "132/132 [==============================] - 35s 268ms/step - loss: 0.9331 - dense_51_loss: 0.5390 - dense_52_loss: 0.3941\n",
      "Epoch 25/50\n",
      "132/132 [==============================] - 35s 266ms/step - loss: 0.9275 - dense_51_loss: 0.5379 - dense_52_loss: 0.3896\n",
      "Epoch 26/50\n",
      "132/132 [==============================] - 35s 265ms/step - loss: 0.9264 - dense_51_loss: 0.5395 - dense_52_loss: 0.3870\n",
      "Epoch 27/50\n",
      "132/132 [==============================] - 35s 266ms/step - loss: 0.9167 - dense_51_loss: 0.5344 - dense_52_loss: 0.3824\n",
      "Epoch 28/50\n",
      "132/132 [==============================] - 35s 267ms/step - loss: 0.9103 - dense_51_loss: 0.5312 - dense_52_loss: 0.3791\n",
      "Epoch 29/50\n",
      "132/132 [==============================] - 35s 268ms/step - loss: 0.8986 - dense_51_loss: 0.5247 - dense_52_loss: 0.3739\n",
      "Epoch 30/50\n",
      "132/132 [==============================] - 36s 269ms/step - loss: 0.8893 - dense_51_loss: 0.5185 - dense_52_loss: 0.3708\n",
      "Epoch 31/50\n",
      "132/132 [==============================] - 36s 271ms/step - loss: 0.8810 - dense_51_loss: 0.5135 - dense_52_loss: 0.3675\n",
      "Epoch 32/50\n",
      "132/132 [==============================] - 36s 270ms/step - loss: 0.8729 - dense_51_loss: 0.5089 - dense_52_loss: 0.3639\n",
      "Epoch 33/50\n",
      "132/132 [==============================] - 36s 269ms/step - loss: 0.8666 - dense_51_loss: 0.5050 - dense_52_loss: 0.3616\n",
      "Epoch 34/50\n",
      "132/132 [==============================] - 35s 266ms/step - loss: 0.8602 - dense_51_loss: 0.5014 - dense_52_loss: 0.3588\n",
      "Epoch 35/50\n",
      "132/132 [==============================] - 35s 266ms/step - loss: 0.8560 - dense_51_loss: 0.4987 - dense_52_loss: 0.3573\n",
      "Epoch 36/50\n",
      "132/132 [==============================] - 36s 269ms/step - loss: 0.8473 - dense_51_loss: 0.4938 - dense_52_loss: 0.3535\n",
      "Epoch 37/50\n",
      "132/132 [==============================] - 35s 265ms/step - loss: 0.8438 - dense_51_loss: 0.4918 - dense_52_loss: 0.3520\n",
      "Epoch 38/50\n",
      "132/132 [==============================] - 36s 272ms/step - loss: 0.8311 - dense_51_loss: 0.4841 - dense_52_loss: 0.3470\n",
      "Epoch 39/50\n",
      "132/132 [==============================] - 36s 274ms/step - loss: 0.8292 - dense_51_loss: 0.4830 - dense_52_loss: 0.3462\n",
      "Epoch 40/50\n",
      "132/132 [==============================] - 35s 268ms/step - loss: 0.8179 - dense_51_loss: 0.4769 - dense_52_loss: 0.3410\n",
      "Epoch 41/50\n",
      "132/132 [==============================] - 36s 274ms/step - loss: 0.8086 - dense_51_loss: 0.4706 - dense_52_loss: 0.3381\n",
      "Epoch 42/50\n",
      "132/132 [==============================] - 36s 272ms/step - loss: 0.8001 - dense_51_loss: 0.4656 - dense_52_loss: 0.3344\n",
      "Epoch 43/50\n",
      "132/132 [==============================] - 36s 272ms/step - loss: 0.7923 - dense_51_loss: 0.4611 - dense_52_loss: 0.3312\n",
      "Epoch 44/50\n",
      "132/132 [==============================] - 36s 273ms/step - loss: 0.7780 - dense_51_loss: 0.4512 - dense_52_loss: 0.3267\n",
      "Epoch 45/50\n",
      "132/132 [==============================] - 36s 271ms/step - loss: 0.7661 - dense_51_loss: 0.4445 - dense_52_loss: 0.3215\n",
      "Epoch 46/50\n",
      "132/132 [==============================] - 36s 272ms/step - loss: 0.7566 - dense_51_loss: 0.4382 - dense_52_loss: 0.3184\n",
      "Epoch 47/50\n",
      "132/132 [==============================] - 36s 269ms/step - loss: 0.7447 - dense_51_loss: 0.4314 - dense_52_loss: 0.3134\n",
      "Epoch 48/50\n",
      "132/132 [==============================] - 36s 270ms/step - loss: 0.7268 - dense_51_loss: 0.4201 - dense_52_loss: 0.3068\n",
      "Epoch 49/50\n",
      "132/132 [==============================] - 36s 271ms/step - loss: 0.7169 - dense_51_loss: 0.4141 - dense_52_loss: 0.3029\n",
      "Epoch 50/50\n",
      "132/132 [==============================] - 36s 269ms/step - loss: 0.7011 - dense_51_loss: 0.4040 - dense_52_loss: 0.2971\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x176b9150588>"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "302"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done\n"
     ]
    }
   ],
   "source": [
    "model = build_model(embedding_matrix, y_aux_train.shape[-1])\n",
    "model.fit(\n",
    "            x_train,\n",
    "            [y_train, y_aux_train],\n",
    "            batch_size=BATCH_SIZE,\n",
    "            epochs=50,\n",
    "            verbose=1,\n",
    "            sample_weight=[sample_weights.values, np.ones_like(sample_weights)],\n",
    "            callbacks = [\n",
    "                LearningRateScheduler(lambda _: 1e-3*(0.55**global_epoch)) # Decayed learning rate make smaller\n",
    "                ]\n",
    "        )\n",
    "pre=model.predict(x_test,batch_size=2048)[0]\n",
    "del model # If a model didn't get deleted Keras will continue training it eventhough build_model() was used to initialize a model\n",
    "gc.collect() # It's a good practice to use gc.collect() once the training is done to free up RAM\n",
    "print('done')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-26T02:58:17.779120Z",
     "start_time": "2019-07-26T02:58:17.747861Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.37590126, 0.36010553, 0.39443555, 0.47206389, 0.37585716,\n",
       "       0.37660376, 0.40683352, 0.46479516, 0.34265305, 0.33180664,\n",
       "       0.39449207, 0.44526081, 0.3398402 , 0.32150702, 0.38474768,\n",
       "       0.44801217, 0.33150918, 0.31096901, 0.40577675, 0.44513621,\n",
       "       0.35884249, 0.34855926, 0.40751236, 0.44901776, 0.39920921,\n",
       "       0.38251756, 0.4241071 , 0.47034284, 0.32807465, 0.32109503,\n",
       "       0.39515136, 0.44363857, 0.38868561, 0.38819343, 0.41867657,\n",
       "       0.46686773, 0.33432597, 0.31538105, 0.39591785, 0.45610078,\n",
       "       0.39844662, 0.40199204, 0.42464707, 0.46176779, 0.4080825 ,\n",
       "       0.39379989, 0.42954928, 0.47249551, 0.36009476, 0.33483327,\n",
       "       0.3878361 , 0.45670795, 0.40726394, 0.39364218, 0.42544565,\n",
       "       0.48026767, 0.36335098, 0.37488028, 0.40097057, 0.46904501,\n",
       "       0.35008594, 0.33776111, 0.39998456, 0.46699422, 0.41549893,\n",
       "       0.4000998 , 0.43301412, 0.47236326, 0.38276006, 0.37097766,\n",
       "       0.42499339, 0.45124439, 0.3899031 , 0.37303503, 0.41135961,\n",
       "       0.47071184, 0.37978952, 0.38412603, 0.42834315, 0.46706361,\n",
       "       0.3855334 , 0.37377825, 0.39975163, 0.47448435, 0.38851164,\n",
       "       0.36253623, 0.41099742, 0.46883777, 0.41348983, 0.40363836,\n",
       "       0.43259525, 0.47506309, 0.39277653, 0.38056505, 0.4190417 ,\n",
       "       0.46304562, 0.35917206, 0.34420599, 0.38596904, 0.46432445,\n",
       "       0.39920921, 0.38251756, 0.4241071 , 0.47034284, 0.39016215,\n",
       "       0.38413419, 0.41183749, 0.4632168 , 0.32535336, 0.31108954,\n",
       "       0.35168031, 0.44223651, 0.39655645, 0.40986085, 0.43024495,\n",
       "       0.47080153, 0.4044007 , 0.39292126, 0.42323441, 0.47460477,\n",
       "       0.35365288, 0.32600525, 0.38784799, 0.46054326, 0.41871664,\n",
       "       0.42673073, 0.43583837, 0.46970721, 0.36598036, 0.33808021,\n",
       "       0.40027689, 0.45387523, 0.40646734, 0.40239328, 0.42944203,\n",
       "       0.46983253, 0.36494268, 0.35157555, 0.39006445, 0.4626802 ,\n",
       "       0.36003397, 0.35968587, 0.39989537, 0.45882521, 0.40226688,\n",
       "       0.39612392, 0.42552099, 0.47058405, 0.40134933, 0.38132763,\n",
       "       0.42011062, 0.4695867 , 0.36197881, 0.3535279 , 0.39655515,\n",
       "       0.45377491, 0.38433334, 0.35872333, 0.40558906, 0.4647181 ,\n",
       "       0.40827083, 0.38769825, 0.41430691, 0.47922845, 0.38867851,\n",
       "       0.37637003, 0.40531058, 0.47320922, 0.40129936, 0.37940115,\n",
       "       0.41704546, 0.46957614, 0.34083713, 0.36491072, 0.38990808,\n",
       "       0.45381809, 0.42046129, 0.41692874, 0.44943175, 0.47427579,\n",
       "       0.32763122, 0.3084114 , 0.36020266, 0.4478972 , 0.34874786,\n",
       "       0.34194226, 0.39338226, 0.45005343, 0.38373003, 0.36336076,\n",
       "       0.44080885, 0.46545994, 0.34155111, 0.32116891, 0.37412057,\n",
       "       0.45841671, 0.43200889, 0.41962278, 0.44225067, 0.48565962,\n",
       "       0.3941354 , 0.38840443, 0.42492318, 0.47462968, 0.36303742,\n",
       "       0.34955898, 0.39829782, 0.45685437, 0.37076823, 0.3640394 ,\n",
       "       0.39966939, 0.46755403, 0.42683374, 0.42018898, 0.45790343,\n",
       "       0.47800991, 0.36702825, 0.35660078, 0.4089415 , 0.44899803,\n",
       "       0.35227107, 0.34450951, 0.39050676, 0.46596334])"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictions = np.average(checkpoint_predictions, weights=weights, axis=0)\n",
    "# submission = pd.DataFrame.from_dict({\n",
    "#     'id': test_df.id,\n",
    "#     'prediction': predictions\n",
    "# })\n",
    "predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-26T02:59:00.296042Z",
     "start_time": "2019-07-26T02:59:00.280434Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3,\n",
       "       3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3,\n",
       "       3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3], dtype=int64)"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pre.argmax(axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-26T03:30:23.881675Z",
     "start_time": "2019-07-26T03:30:23.850432Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([3, 3, 2, 2, 2, 2, 3, 3, 3, 2, 1, 3, 3, 3, 3, 3, 3, 2, 3, 3, 3, 3,\n",
       "       3, 2, 3, 3, 3, 2, 3, 2, 3, 1, 3, 2, 3, 3, 3, 3, 3, 3, 3, 3, 3, 1,\n",
       "       2, 3, 2, 2, 3, 3, 3, 3, 3, 2, 2, 3], dtype=int64)"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pre.argmax(axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-26T02:47:22.860253Z",
     "start_time": "2019-07-26T02:47:22.829121Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0     3\n",
       "1     3\n",
       "2     2\n",
       "3     2\n",
       "4     2\n",
       "5     2\n",
       "6     3\n",
       "7     2\n",
       "8     3\n",
       "9     2\n",
       "10    1\n",
       "11    3\n",
       "12    3\n",
       "13    3\n",
       "14    1\n",
       "15    3\n",
       "16    3\n",
       "17    2\n",
       "18    3\n",
       "19    3\n",
       "20    0\n",
       "21    2\n",
       "22    3\n",
       "23    3\n",
       "24    3\n",
       "25    3\n",
       "26    1\n",
       "27    0\n",
       "28    1\n",
       "29    2\n",
       "30    0\n",
       "31    1\n",
       "32    3\n",
       "33    2\n",
       "34    2\n",
       "35    1\n",
       "36    2\n",
       "37    3\n",
       "38    3\n",
       "39    2\n",
       "40    1\n",
       "41    0\n",
       "42    0\n",
       "43    1\n",
       "44    2\n",
       "45    0\n",
       "46    2\n",
       "47    2\n",
       "48    3\n",
       "49    3\n",
       "50    3\n",
       "51    0\n",
       "52    3\n",
       "53    2\n",
       "54    2\n",
       "55    3\n",
       "Name: target, dtype: int64"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test['target']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-26T14:25:09.166956Z",
     "start_time": "2019-07-26T14:25:09.135722Z"
    }
   },
   "outputs": [],
   "source": [
    "SYMBOLS_TO_ISOLATE = '.,?!-;*\"‚Ä¶:‚Äî()%#$&_/@Ôºº„Éªœâ+=‚Äù‚Äú[]^‚Äì>\\\\¬∞<~‚Ä¢‚â†‚Ñ¢Àà ä…í‚àû¬ß{}¬∑œÑŒ±‚ù§‚ò∫…°|¬¢‚ÜíÃ∂`‚ù•‚îÅ‚î£‚î´‚îóÔºØ‚ñ∫‚òÖ¬©‚Äï…™‚úî¬Æ\\x96\\x92‚óè¬£‚ô•‚û§¬¥¬π‚òï‚âà√∑‚ô°‚óê‚ïë‚ñ¨‚Ä≤…îÀê‚Ç¨€©€û‚Ä†Œº‚úí‚û•‚ïê‚òÜÀå‚óÑ¬Ω ªœÄŒ¥Œ∑ŒªœÉŒµœÅŒΩ É‚ú¨Ôº≥ÔºµÔº∞Ôº•Ôº≤Ôº©Ôº¥‚òª¬±‚ôç¬µ¬∫¬æ‚úì‚óæÿüÔºé‚¨Ö‚ÑÖ¬ª–í–∞–≤‚ù£‚ãÖ¬ø¬¨‚ô´Ôº£Ôº≠Œ≤‚ñà‚ñì‚ñí‚ñë‚áí‚≠ê‚Ä∫¬°‚ÇÇ‚ÇÉ‚ùß‚ñ∞‚ñî‚óû‚ñÄ‚ñÇ‚ñÉ‚ñÑ‚ñÖ‚ñÜ‚ñá‚ÜôŒ≥ÃÑ‚Ä≥‚òπ‚û°¬´œÜ‚Öì‚Äû‚úãÔºö¬•Ã≤ÃÖÃÅ‚àô‚Äõ‚óá‚úè‚ñ∑‚ùì‚ùó¬∂ÀöÀôÔºâ—Å–∏ ø‚ú®„ÄÇ…ë\\x80‚óïÔºÅÔºÖ¬Ø‚àíÔ¨ÇÔ¨Å‚ÇÅ¬≤ å¬º‚Å¥‚ÅÑ‚ÇÑ‚å†‚ô≠‚úò‚ï™‚ñ∂‚ò≠‚ú≠‚ô™‚òî‚ò†‚ôÇ‚òÉ‚òé‚úà‚úå‚ú∞‚ùÜ‚òô‚óã‚Ä£‚öìÂπ¥‚àé‚Ñí‚ñ™‚ñô‚òè‚ÖõÔΩÉÔΩÅÔΩì«Ä‚ÑÆ¬∏ÔΩó‚Äö‚àº‚Äñ‚Ñ≥‚ùÑ‚Üê‚òº‚ãÜ í‚äÇ„ÄÅ‚Öî¬®Õ°‡πè‚öæ‚öΩŒ¶√óŒ∏Ôø¶ÔºüÔºà‚ÑÉ‚è©‚òÆ‚ö†Êúà‚úä‚ùå‚≠ï‚ñ∏‚ñ†‚áå‚òê‚òë‚ö°‚òÑ«´‚ï≠‚à©‚ïÆÔºå‰æãÔºû ï…êÃ£Œî‚ÇÄ‚úû‚îà‚ï±‚ï≤‚ñè‚ñï‚îÉ‚ï∞‚ñä‚ñã‚ïØ‚î≥‚îä‚â•‚òí‚Üë‚òù…π‚úÖ‚òõ‚ô©‚òûÔº°Ôº™Ôº¢‚óî‚ó°‚Üì‚ôÄ‚¨ÜÃ±‚Ñè\\x91‚†ÄÀ§‚ïö‚Ü∫‚á§‚àè‚úæ‚ó¶‚ô¨¬≥„ÅÆÔΩúÔºè‚àµ‚à¥‚àöŒ©¬§‚òú‚ñ≤‚Ü≥‚ñ´‚Äø‚¨á‚úßÔΩèÔΩñÔΩçÔºçÔºíÔºêÔºòÔºá‚Ä∞‚â§‚àïÀÜ‚öú‚òÅ'\n",
    "SYMBOLS_TO_REMOVE = '\\nüçï\\rüêµüòë\\xa0\\ue014\\t\\uf818\\uf04a\\xadüò¢üê∂Ô∏è\\uf0e0üòúüòéüëä\\u200b\\u200eüòÅÿπÿØŸàŸäŸáÿµŸÇÿ£ŸÜÿßÿÆŸÑŸâÿ®ŸÖÿ∫ÿ±üòçüíñüíµ–ïüëéüòÄüòÇ\\u202a\\u202cüî•üòÑüèªüí•·¥ç è Ä·¥á…¥·¥Ö·¥è·¥Ä·¥ã ú·¥ú ü·¥õ·¥Ñ·¥ò ô“ì·¥ä·¥°…¢üòãüëè◊©◊ú◊ï◊ù◊ë◊ôüò±‚Äº\\x81„Ç®„É≥„Ç∏ÊïÖÈöú\\u2009üöå·¥µÕûüåüüòäüò≥üòßüôÄüòêüòï\\u200füëçüòÆüòÉüòò◊ê◊¢◊õ◊óüí©üíØ‚õΩüöÑüèº‡Æúüòñ·¥†üö≤‚Äêüòüüòàüí™üôèüéØüåπüòáüíîüò°\\x7füëå·ºê·Ω∂ŒÆŒπ·Ω≤Œ∫·ºÄŒØ·øÉ·º¥ŒæüôÑÔº®üò†\\ufeff\\u2028üòâüò§‚õ∫üôÇ\\u3000ÿ™ÿ≠ŸÉÿ≥ÿ©üëÆüíôŸÅÿ≤ÿ∑üòèüçæüéâüòû\\u2008üèæüòÖüò≠üëªüò•üòîüòìüèΩüéÜüçªüçΩüé∂üå∫ü§îüò™\\x08‚Äëüê∞üêáüê±üôÜüò®üôÉüíïùòäùò¶ùò≥ùò¢ùòµùò∞ùò§ùò∫ùò¥ùò™ùòßùòÆùò£üíóüíöÂú∞ÁçÑË∞∑—É–ª–∫–Ω–ü–æ–ê–ùüêæüêïüòÜ◊îüîóüöΩÊ≠åËàû‰ºéüôàüò¥üèøü§óüá∫üá∏–ºœÖ—Ç—ï‚§µüèÜüéÉüò©\\u200aüå†üêüüí´üí∞üíé—ç–ø—Ä–¥\\x95üñêüôÖ‚õ≤üç∞ü§êüëÜüôå\\u2002üíõüôÅüëÄüôäüôâ\\u2004À¢·µí ≥ ∏·¥º·¥∑·¥∫ ∑·µó ∞·µâ·µò\\x13üö¨ü§ì\\ue602üòµŒ¨ŒøœåœÇŒ≠·Ω∏◊™◊û◊ì◊£◊†◊®◊ö◊¶◊òüòíÕùüÜïüëÖüë•üëÑüîÑüî§üëâüë§üë∂üë≤üîõüéì\\uf0b7\\uf04c\\x9f\\x10ÊàêÈÉΩüò£‚è∫üòåü§ëüåèüòØ–µ—Öüò≤·º∏·æ∂·ΩÅüíûüöìüîîüìöüèÄüëê\\u202düí§üçá\\ue613Â∞èÂúüË±Üüè°‚ùî‚Åâ\\u202füë†„Äã‡§ï‡§∞‡•ç‡§Æ‡§æüáπüáºüå∏Ëî°Ëã±Êñáüåûüé≤„É¨„ÇØ„Çµ„ÇπüòõÂ§ñÂõΩ‰∫∫ÂÖ≥Á≥ª–°–±üíãüíÄüéÑüíúü§¢ŸêŸé—å—ã–≥—è‰∏çÊòØ\\x9c\\x9düóë\\u2005üíÉüì£üëø‡ºº„Å§‡ºΩüò∞·∏∑–ó–∑‚ñ±—ÜÔøºü§£ÂçñÊ∏©Âì•ÂçéËÆÆ‰ºö‰∏ãÈôç‰Ω†Â§±ÂéªÊâÄÊúâÁöÑÈí±Âä†ÊãøÂ§ßÂùèÁ®éÈ™óÂ≠êüêù„ÉÑüéÖ\\x85üç∫ÿ¢ÿ•ÿ¥ÿ°üéµüåéÕü·ºîÊ≤πÂà´ÂÖãü§°ü§•üò¨ü§ß–π\\u2003üöÄü§¥ ≤—à—á–ò–û–†–§–î–Ø–ú—é–∂üòùüñë·Ωê·ΩªœçÁâπÊÆä‰ΩúÊà¶Áæ§—âüí®ÂúÜÊòéÂõ≠◊ß‚Ñêüèàüò∫üåç‚èè·ªáüçîüêÆüçÅüçÜüçëüåÆüåØü§¶\\u200dùìíùì≤ùìøùìµÏïàÏòÅÌïòÏÑ∏Ïöî–ñ—ô–ö—õüçÄüò´ü§§·ø¶ÊàëÂá∫ÁîüÂú®‰∫ÜÂèØ‰ª•ËØ¥ÊôÆÈÄöËØùÊ±âËØ≠Â•ΩÊûÅüéºüï∫üç∏ü•ÇüóΩüéáüéäüÜòü§†üë©üñíüö™Â§©‰∏ÄÂÆ∂‚ö≤\\u2006‚ö≠‚öÜ‚¨≠‚¨Ø‚èñÊñ∞‚úÄ‚ïåüá´üá∑üá©üá™üáÆüá¨üáßüò∑üá®üá¶–•–®üåê\\x1fÊùÄÈ∏°ÁªôÁå¥Áúã Åùó™ùóµùó≤ùóªùòÜùóºùòÇùóøùóÆùóπùó∂ùòáùóØùòÅùó∞ùòÄùòÖùóΩùòÑùó±üì∫œñ\\u2000“Ø’Ω·¥¶·é•“ªÕ∫\\u2007’∞\\u2001…©ÔΩôÔΩÖ‡µ¶ÔΩå∆ΩÔΩàùêìùê°ùêûùê´ùêÆùêùùêöùêÉùêúùê©ùê≠ùê¢ùê®ùêß∆Ñ·¥®◊ü·ëØ‡ªêŒ§·èß‡Ø¶–Ü·¥ë‹Åùê¨ùê∞ùê≤ùêõùê¶ùêØùêëùêôùê£ùêáùêÇùêòùüé‘ú–¢·óû‡±¶„Äî·é´ùê≥ùêîùê±ùüîùüìùêÖüêãÔ¨Éüíòüíì—ëùò•ùòØùò∂üíêüåãüåÑüåÖùô¨ùôñùô®ùô§ùô£ùô°ùôÆùôòùô†ùôöùôôùôúùôßùô•ùô©ùô™ùôóùôûùôùùôõüë∫üê∑‚ÑãùêÄùê•ùê™üö∂ùô¢·ºπü§òÕ¶üí∏ÿ¨Ìå®Ìã∞Ôº∑ùôá·µªüëÇüëÉ…úüé´\\uf0a7–ë–£—ñüö¢üöÇ‡™ó‡´Å‡™ú‡™∞‡™æ‡™§‡´Ä·øÜüèÉùì¨ùìªùì¥ùìÆùìΩùìº‚òòÔ¥æÃØÔ¥ø‚ÇΩ\\ue807ùëªùíÜùíçùíïùíâùíìùíñùíÇùíèùíÖùíîùíéùíóùíäüëΩüòô\\u200c–õ‚Äíüéæüëπ‚éåüèí‚õ∏ÂÖ¨ÂØìÂÖªÂÆ†Áâ©ÂêóüèÑüêÄüöëü§∑ÊìçÁæéùíëùíöùíêùë¥ü§ôüêíÊ¨¢ËøéÊù•Âà∞ÈòøÊãâÊñØ◊°◊§ùô´üêàùíåùôäùô≠ùôÜùôãùôçùòºùôÖÔ∑ªü¶ÑÂ∑®Êî∂Ëµ¢ÂæóÁôΩÈ¨ºÊÑ§ÊÄíË¶Å‰π∞È¢ù·∫Ωüöóüê≥ùüèùêüùüñùüëùüïùíÑùüóùê†ùôÑùôÉüëáÈîüÊñ§Êã∑ùó¢ùü≥ùü±ùü¨‚¶Å„Éû„É´„Éè„Éã„ÉÅ„É≠Ê†™ÂºèÁ§æ‚õ∑ÌïúÍµ≠Ïñ¥„Ñ∏„ÖìÎãàÕú ñùòøùôî‚Çµùí©‚ÑØùíæùìÅùí∂ùìâùìáùìäùìÉùìàùìÖ‚Ñ¥ùíªùíΩùìÄùìåùí∏ùìéùôèŒ∂ùôüùòÉùó∫ùüÆùü≠ùüØùü≤üëãü¶äÂ§ö‰º¶üêΩüéªüéπ‚õìüèπüç∑ü¶Ü‰∏∫Âíå‰∏≠ÂèãË∞äÁ•ùË¥∫‰∏éÂÖ∂ÊÉ≥Ë±°ÂØπÊ≥ïÂ¶ÇÁõ¥Êé•ÈóÆÁî®Ëá™Â∑±ÁåúÊú¨‰º†ÊïôÂ£´Ê≤°ÁßØÂîØËÆ§ËØÜÂü∫Áù£ÂæíÊõæÁªèËÆ©Áõ∏‰ø°ËÄ∂Á®£Â§çÊ¥ªÊ≠ªÊÄ™‰ªñ‰ΩÜÂΩì‰ª¨ËÅä‰∫õÊîøÊ≤ªÈ¢òÊó∂ÂÄôÊàòËÉúÂõ†Âú£ÊääÂÖ®Â†ÇÁªìÂ©öÂ≠©ÊÅêÊÉß‰∏îÊ†óË∞ìËøôÊ†∑Ëøò‚ôæüé∏ü§ïü§í‚õëüéÅÊâπÂà§Ê£ÄËÆ®üèùü¶Åüôãüò∂Ï•êÏä§ÌÉ±Ìä∏Î§ºÎèÑÏÑùÏú†Í∞ÄÍ≤©Ïù∏ÏÉÅÏù¥Í≤ΩÏ†úÌô©ÏùÑÎ†µÍ≤åÎßåÎì§ÏßÄÏïäÎ°ùÏûòÍ¥ÄÎ¶¨Ìï¥ÏïºÌï©Îã§Ï∫êÎÇòÏóêÏÑúÎåÄÎßàÏ¥àÏôÄÌôîÏïΩÍ∏àÏùòÌíàÎü∞ÏÑ±Î∂ÑÍ∞àÎïåÎäîÎ∞òÎìúÏãúÌóàÎêúÏÇ¨Ïö©üî´üëÅÂá∏·Ω∞üí≤üóØùôà·ºåùíáùíàùíòùíÉùë¨ùë∂ùïæùñôùñóùñÜùñéùñåùñçùñïùñäùñîùñëùñâùñìùñêùñúùñûùñöùñáùïøùñòùñÑùñõùñíùñãùñÇùï¥ùñüùñàùï∏üëëüöøüí°Áü•ÂΩºÁôæ\\uf005ùôÄùíõùë≤ùë≥ùëæùíãùüíüò¶ùôíùòæùòΩüèêùò©ùò®·Ωº·πëùë±ùëπùë´ùëµùë™üá∞üáµüëæ·ìá·íß·î≠·êÉ·êß·ê¶·ë≥·ê®·ìÉ·ìÇ·ë≤·ê∏·ë≠·ëé·ìÄ·ê£üêÑüéàüî®üêéü§ûüê∏üíüüé∞üåùüõ≥ÁÇπÂáªÊü•Áâàüç≠ùë•ùë¶ùëßÔºÆÔºßüë£\\uf020„Å£üèâ—Ñüí≠üé•Œûüê¥üë®ü§≥ü¶ç\\x0büç©ùëØùííüòóùüêüèÇüë≥üçóüïâüê≤⁄Ü€åùëÆùóïùó¥üçíÍú•‚≤£‚≤èüêë‚è∞ÈâÑ„É™‰∫ã‰ª∂—óüíä„Äå„Äç\\uf203\\uf09a\\uf222\\ue608\\uf202\\uf099\\uf469\\ue607\\uf410\\ue600ÁáªË£Ω„Ç∑ËôöÂÅΩÂ±ÅÁêÜÂ±à–ìùë©ùë∞ùíÄùë∫üå§ùó≥ùóúùóôùó¶ùóßüçä·Ω∫·ºà·º°œá·øñŒõ‚§èüá≥ùíôœà’Å’¥’•’º’°’µ’´’∂÷Ä÷Ç’§’±ÂÜ¨Ëá≥·ΩÄùíÅüîπü§öüçéùë∑üêÇüíÖùò¨ùò±ùò∏ùò∑ùòêùò≠ùòìùòñùòπùò≤ùò´⁄©Œíœéüí¢ŒúŒüŒùŒëŒïüá±‚ô≤ùùà‚Ü¥üíí‚äò»ªüö¥üñïüñ§ü•òüìçüëà‚ûïüö´üé®üåëüêªùêéùêçùêäùë≠ü§ñüééüòºüï∑ÔΩáÔΩíÔΩéÔΩîÔΩâÔΩÑÔΩïÔΩÜÔΩÇÔΩãùü∞üá¥üá≠üáªüá≤ùóûùó≠ùóòùó§üëºüìâüçüüç¶üåàüî≠„Ääüêäüêç\\uf10a·Éö⁄°üê¶\\U0001f92f\\U0001f92aüê°üí≥·º±üôáùó∏ùóüùó†ùó∑ü•ú„Åï„Çà„ÅÜ„Å™„Çâüîº'\n",
    "ISOLATE_DICT = {ord(c):f' {c} ' for c in SYMBOLS_TO_ISOLATE}\n",
    "REMOVE_DICT = {ord(c):f'' for c in SYMBOLS_TO_REMOVE}\n",
    "CHARS_TO_REMOVE = '!\"#$%&()*+,-./:;<=>?@[\\\\]^_`{|}~\\t\\n‚Äú‚Äù‚Äô\\'‚àûŒ∏√∑Œ±‚Ä¢√†‚àíŒ≤‚àÖ¬≥œÄ‚Äò‚Çπ¬¥¬∞¬£‚Ç¨\\√ó‚Ñ¢‚àö¬≤‚Äî'\n",
    "CONTRACTION_MAPPING = {\"ain't\": \"is not\", \"aren't\": \"are not\",\"can't\": \"cannot\", \"'cause\": \"because\", \"could've\": \"could have\", \"couldn't\": \"could not\", \"didn't\": \"did not\",  \"doesn't\": \"does not\", \"don't\": \"do not\", \"hadn't\": \"had not\", \"hasn't\": \"has not\", \"haven't\": \"have not\", \"he'd\": \"he would\",\"he'll\": \"he will\", \"he's\": \"he is\", \"how'd\": \"how did\", \"how'd'y\": \"how do you\", \"how'll\": \"how will\", \"how's\": \"how is\",  \"I'd\": \"I would\", \"I'd've\": \"I would have\", \"I'll\": \"I will\", \"I'll've\": \"I will have\",\"I'm\": \"I am\", \"I've\": \"I have\", \"i'd\": \"i would\", \"i'd've\": \"i would have\", \"i'll\": \"i will\",  \"i'll've\": \"i will have\",\"i'm\": \"i am\", \"i've\": \"i have\", \"isn't\": \"is not\", \"it'd\": \"it would\", \"it'd've\": \"it would have\", \"it'll\": \"it will\", \"it'll've\": \"it will have\",\"it's\": \"it is\", \"let's\": \"let us\", \"ma'am\": \"madam\", \"mayn't\": \"may not\", \"might've\": \"might have\",\"mightn't\": \"might not\",\"mightn't've\": \"might not have\", \"must've\": \"must have\", \"mustn't\": \"must not\", \"mustn't've\": \"must not have\", \"needn't\": \"need not\", \"needn't've\": \"need not have\",\"o'clock\": \"of the clock\", \"oughtn't\": \"ought not\", \"oughtn't've\": \"ought not have\", \"shan't\": \"shall not\", \"sha'n't\": \"shall not\", \"shan't've\": \"shall not have\", \"she'd\": \"she would\", \"she'd've\": \"she would have\", \"she'll\": \"she will\", \"she'll've\": \"she will have\", \"she's\": \"she is\", \"should've\": \"should have\", \"shouldn't\": \"should not\", \"shouldn't've\": \"should not have\", \"so've\": \"so have\",\"so's\": \"so as\", \"this's\": \"this is\",\"that'd\": \"that would\", \"that'd've\": \"that would have\", \"that's\": \"that is\", \"there'd\": \"there would\", \"there'd've\": \"there would have\", \"there's\": \"there is\", \"here's\": \"here is\",\"they'd\": \"they would\", \"they'd've\": \"they would have\", \"they'll\": \"they will\", \"they'll've\": \"they will have\", \"they're\": \"they are\", \"they've\": \"they have\", \"to've\": \"to have\", \"wasn't\": \"was not\", \"we'd\": \"we would\", \"we'd've\": \"we would have\", \"we'll\": \"we will\", \"we'll've\": \"we will have\", \"we're\": \"we are\", \"we've\": \"we have\", \"weren't\": \"were not\", \"what'll\": \"what will\", \"what'll've\": \"what will have\", \"what're\": \"what are\",  \"what's\": \"what is\", \"what've\": \"what have\", \"when's\": \"when is\", \"when've\": \"when have\", \"where'd\": \"where did\", \"where's\": \"where is\", \"where've\": \"where have\", \"who'll\": \"who will\", \"who'll've\": \"who will have\", \"who's\": \"who is\", \"who've\": \"who have\", \"why's\": \"why is\", \"why've\": \"why have\", \"will've\": \"will have\", \"won't\": \"will not\", \"won't've\": \"will not have\", \"would've\": \"would have\", \"wouldn't\": \"would not\", \"wouldn't've\": \"would not have\", \"y'all\": \"you all\", \"y'all'd\": \"you all would\",\"y'all'd've\": \"you all would have\",\"y'all're\": \"you all are\",\"y'all've\": \"you all have\",\"you'd\": \"you would\", \"you'd've\": \"you would have\", \"you'll\": \"you will\", \"you'll've\": \"you will have\", \"you're\": \"you are\", \"you've\": \"you have\" }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
