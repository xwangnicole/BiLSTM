{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-26T20:51:06.701930Z",
     "start_time": "2019-07-26T20:51:06.655061Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting C:/Users/nwang/Desktop/nlp/glove/BiLSTMmodel_aux.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile C:/Users/nwang/Desktop/nlp/glove/BiLSTMmodel_aux.py\n",
    "\n",
    "# 'LSTM module '\n",
    "\n",
    "# __author__ = 'Nicole Wang'\n",
    "\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "import re, string\n",
    "import sys\n",
    "import pickle\n",
    "import warnings\n",
    "import matplotlib.pyplot as plt\n",
    "import h5py\n",
    "from keras.models import model_from_json\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "rf = RandomForestClassifier()\n",
    "\n",
    "from keras.models import Model, Sequential\n",
    "from keras.layers import Input, Dense, Activation\n",
    "import csv\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from keras.layers import Dense, Input, Dropout, LSTM, Activation, Embedding, concatenate\n",
    "from keras.layers import Bidirectional, GlobalMaxPool1D\n",
    "from keras.preprocessing import sequence\n",
    "np.random.seed(1)\n",
    "maxlen=24\n",
    "\n",
    "from stop_words import get_stop_words\n",
    "stopwords = get_stop_words('en')\n",
    "lemmatizer = nltk.WordNetLemmatizer()\n",
    "transtbl = str.maketrans(string.punctuation, ' '*len(string.punctuation))\n",
    "\n",
    "from util import read_glove_vec\n",
    "word_to_index, word_to_vec_map = read_glove_vec('C:/Users/nwang/Desktop/nlp/glove.6B.50d.txt')\n",
    "\n",
    "class BiLSTMmodel:\n",
    "    probability=None\n",
    "    prediction=None\n",
    "    pretrain_probability=None\n",
    "\n",
    "    \n",
    "    def text_clean(self,text):\n",
    "        if not isinstance(text,float) :\n",
    "            text=str(text)\n",
    "            text = ' '.join([appos[we] if we in appos else we for we in text.split()])\n",
    "            text =text.translate(transtbl)\n",
    "    #         tokens = [lemmatizer.lemmatize(t.lower(),'v')\n",
    "    #                  for t in nltk.word_tokenize(text)\n",
    "    #                  if t.lower() not in stopwords]\n",
    "            return ' '.join(text.split())\n",
    "        else:\n",
    "            return np.nan\n",
    "    \n",
    "    \n",
    "    \n",
    "    def _pretrained_embedding_layer(self,word_to_vec_map, word_to_index):\n",
    "        vocab_len = len(word_to_index) + 1  \n",
    "        emb_dim = list(word_to_vec_map.values())[0].shape[0]\n",
    "\n",
    "        emb_matrix = np.zeros((vocab_len, emb_dim))\n",
    "\n",
    "        for word, index in word_to_index.items():\n",
    "            emb_matrix[index, :] = word_to_vec_map[word]\n",
    "\n",
    "\n",
    "        return Embedding(\n",
    "            input_dim=vocab_len, \n",
    "            output_dim=emb_dim, \n",
    "            trainable=False, \n",
    "            weights=[emb_matrix])\n",
    "    \n",
    "    def _sentences_to_indice(self, X, word_to_index, max_len):\n",
    "        m = X.shape[0]\n",
    "        X_indices = np.zeros((m, max_len))\n",
    "\n",
    "        for i in range(m):\n",
    "            sentence_words = X[i].lower().split()\n",
    "            j = 0\n",
    "            for w in sentence_words:\n",
    "                try:\n",
    "                    X_indices[i, j] = word_to_index[w]\n",
    "                    j = j + 1\n",
    "                except:\n",
    "                    X_indices[i, j] = word_to_index['unk']\n",
    "                    j = j + 1\n",
    "\n",
    "        return X_indices\n",
    "    \n",
    "    def _convert_to_one_hot(self,Y, C):\n",
    "        Y = np.eye(C)[Y.reshape(-1)]\n",
    "        return Y\n",
    "    \n",
    "    def _mmodel(self,input_shape, word_to_vec_map, word_to_index,d=3):\n",
    "        # Input layer\n",
    "        sentence_indices = Input(shape=input_shape, dtype='int32')\n",
    "        aux_indices = Input(shape=(d,), dtype='float32')\n",
    "\n",
    "        # Embedding layer\n",
    "        embedding_layer = self._pretrained_embedding_layer(word_to_vec_map, word_to_index)\n",
    "        embeddings = embedding_layer(sentence_indices)   \n",
    "\n",
    "        # 2-layer LSTM\n",
    "        X = Bidirectional(LSTM(128, return_sequences=True, recurrent_dropout=0.5))(embeddings)  # N->N RNN\n",
    "        X = Dropout(0.5)(X)\n",
    "        X = Bidirectional(LSTM(128, recurrent_dropout=0.5))(X)  # N -> 1 RNN\n",
    "        X = Dropout(0.5)(X)\n",
    "        X = concatenate([X,aux_indices])\n",
    "        X = Dense(4, activation='softmax')(X)\n",
    "        \n",
    "\n",
    "        # Create and return model\n",
    "        model = Model(inputs=[sentence_indices,aux_indices], outputs=X)\n",
    "\n",
    "        return model\n",
    "    \n",
    "    def fit_predict(self, X_train, X_test, Y_train,Y_test,train_aux,test_aux,d=3,cn=4, epochs=50, batch_size=32,shuffle=True):\n",
    "        sample_weights = np.ones(len(X_train), dtype=np.float32)\n",
    "        sw=pd.DataFrame(Y_train,columns=['target'])\n",
    "        sample_weights *= sw.groupby(['target'])['target'].transform('count')\n",
    "        sample_weights/=sample_weights.mean()\n",
    "        \n",
    "        X_train_indices = self._sentences_to_indice(X_train, word_to_index, maxlen)\n",
    "        X_test_indices = self._sentences_to_indice(X_test, word_to_index, maxlen)\n",
    "        model = self._mmodel((maxlen,), word_to_vec_map, word_to_index,d)\n",
    "        print(model.summary())\n",
    "        \n",
    "        model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "        Y_train_oh = self._convert_to_one_hot(Y_train, C = cn)\n",
    "        Y_test_oh = self._convert_to_one_hot(Y_test, C = cn)\n",
    "        history = model.fit([X_train_indices,train_aux], \n",
    "                        Y_train_oh, \n",
    "                        validation_split=0.2,\n",
    "                        epochs = epochs, \n",
    "                        batch_size = batch_size, \n",
    "                        sample_weight=[sample_weights.values],\n",
    "                        shuffle=shuffle)\n",
    "        self.probability=model.predict([X_test_indices,test_aux])\n",
    "        self.prediction=self.probability.argmax(axis=1)\n",
    "        loss, acc = model.evaluate([X_test_indices,test_aux], Y_test_oh)\n",
    "        plt.plot(history.history['loss'])\n",
    "        plt.plot(history.history['acc'])\n",
    "        \n",
    "        \n",
    "        print('----------------------------------------TEST ACCURACY: '+str(acc))\n",
    "        with open('C:/Users/nwang/Desktop/nlp/model/bilstm_model.json', 'w') as fp:\n",
    "            fp.write(model.to_json())\n",
    "        model.save_weights('C:/Users/nwang/Desktop/nlp/model/bilstm_model.h5')\n",
    "    \n",
    "    def pretrain(self, textarray,aux):\n",
    "        path1='C:/Users/nwang/Desktop/nlp/model/bilstm_model.json'\n",
    "        path2='C:/Users/nwang/Desktop/nlp/model/bilstm_model.h5'\n",
    "        with open(path1,'r') as fp:\n",
    "            xmodel=model_from_json(fp.read())\n",
    "        xmodel.load_weights(path2)\n",
    "\n",
    "        xmodel.compile(\n",
    "            loss='categorical_crossentropy', \n",
    "            optimizer='adam', \n",
    "        #     metrics=[auc])\n",
    "            metrics=['accuracy'])\n",
    "        \n",
    "        one_index= self._sentences_to_indice(textarray, word_to_index, maxlen)\n",
    "        ar=xmodel.predict([one_index,aux])\n",
    "        self.pretrain_probability=ar\n",
    "        return ar.argmax(axis=1)\n",
    "\n",
    "    \n",
    "    \n",
    "    \n",
    "appos = {\n",
    "\n",
    "\"aren't\" : \"are not\",\n",
    "\n",
    "\"can't\" : \"cannot\",\n",
    "\n",
    "\"couldn't\" : \"could not\",\n",
    "\n",
    "\"didn't\" : \"did not\",\n",
    "\n",
    "\"doesn't\" : \"does not\",\n",
    "\n",
    "\"don't\" : \"do not\",\n",
    "\n",
    "\"hadn't\" : \"had not\",\n",
    "\n",
    "\"hasn't\" : \"has not\",\n",
    "\n",
    "\"haven't\" : \"have not\",\n",
    "\n",
    "\"he'd\" : \"he would\",\n",
    "\n",
    "\"he'll\" : \"he will\",\n",
    "\n",
    "\"he's\" : \"he is\",\n",
    "\n",
    "\"i'd\" : \"I would\",\n",
    "\n",
    "\"i'd\" : \"I had\",\n",
    "\n",
    "\"i'll\" : \"I will\",\n",
    "\n",
    "\"i'm\" : \"I am\",\n",
    "\n",
    "\"isn't\" : \"is not\",\n",
    "\n",
    "\"it's\" : \"it is\",\n",
    "\n",
    "\"it'll\":\"it will\",\n",
    "\n",
    "\"i've\" : \"I have\",\n",
    "\n",
    "\"let's\" : \"let us\",\n",
    "\n",
    "\"mightn't\" : \"might not\",\n",
    "\n",
    "\"mustn't\" : \"must not\",\n",
    "\n",
    "\"shan't\" : \"shall not\",\n",
    "\n",
    "\"she'd\" : \"she would\",\n",
    "\n",
    "\"she'll\" : \"she will\",\n",
    "\n",
    "\"she's\" : \"she is\",\n",
    "\n",
    "\"shouldn't\" : \"should not\",\n",
    "\n",
    "\"that's\" : \"that is\",\n",
    "\n",
    "\"there's\" : \"there is\",\n",
    "\n",
    "\"they'd\" : \"they would\",\n",
    "\n",
    "\"they'll\" : \"they will\",\n",
    "\n",
    "\"they're\" : \"they are\",\n",
    "\n",
    "\"they've\" : \"they have\",\n",
    "\n",
    "\"we'd\" : \"we would\",\n",
    "\n",
    "\"we're\" : \"we are\",\n",
    "\n",
    "\"weren't\" : \"were not\",\n",
    "\n",
    "\"we've\" : \"we have\",\n",
    "\n",
    "\"what'll\" : \"what will\",\n",
    "\n",
    "\"what're\" : \"what are\",\n",
    "\n",
    "\"what's\" : \"what is\",\n",
    "\n",
    "\"what've\" : \"what have\",\n",
    "\n",
    "\"where's\" : \"where is\",\n",
    "\n",
    "\"who'd\" : \"who would\",\n",
    "\n",
    "\"who'll\" : \"who will\",\n",
    "\n",
    "\"who're\" : \"who are\",\n",
    "\n",
    "\"who's\" : \"who is\",\n",
    "\n",
    "\"who've\" : \"who have\",\n",
    "\n",
    "\"won't\" : \"will not\",\n",
    "\n",
    "\"wouldn't\" : \"would not\",\n",
    "\n",
    "\"you'd\" : \"you would\",\n",
    "\n",
    "\"you'll\" : \"you will\",\n",
    "\n",
    "\"you're\" : \"you are\",\n",
    "\n",
    "\"you've\" : \"you have\",\n",
    "\n",
    "\"'re\": \" are\",\n",
    "\n",
    "\"wasn't\": \"was not\",\n",
    "\n",
    "\"we'll\":\" will\",\n",
    "\n",
    "\"didn't\": \"did not\"\n",
    "\n",
    "}\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-24T01:01:24.472600Z",
     "start_time": "2019-07-24T01:01:24.441365Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing C:/Users/nwang/Desktop/nlp/glove/util.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile C:/Users/nwang/Desktop/nlp/glove/util.py\n",
    "\n",
    "# __author__ = 'Nicole Wang'\n",
    "import numpy as np\n",
    "\n",
    "def read_glove_vec(glove_file):\n",
    "    with open(glove_file,encoding=\"utf8\") as f:\n",
    "        words = set()\n",
    "        word_to_vec_map = {}\n",
    "        for line in f:\n",
    "            line = line.strip().split()\n",
    "            curr_word = line[0]\n",
    "            words.add(curr_word)\n",
    "            word_to_vec_map[curr_word] = np.array(line[1:], dtype=np.float64)\n",
    "\n",
    "        i = 1\n",
    "        words_to_index = {}\n",
    "        for w in sorted(words):\n",
    "            words_to_index[w] = i\n",
    "            i = i + 1\n",
    "    return words_to_index, word_to_vec_map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-24T01:26:41.994090Z",
     "start_time": "2019-07-24T01:26:41.572195Z"
    }
   },
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-24T01:34:06.203147Z",
     "start_time": "2019-07-24T01:34:04.865053Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.FloatTensor(word_embedding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-24T01:33:58.073270Z",
     "start_time": "2019-07-24T01:33:58.057645Z"
    }
   },
   "outputs": [],
   "source": [
    "word_embedding=np.zeros(10, dtype=\"float\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
